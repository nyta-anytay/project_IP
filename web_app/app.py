"""
Streamlit –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–∞—Å–æ–∫
"""
import streamlit as st
import cv2
import numpy as np
from PIL import Image
import pickle
import json
import os
import warnings
warnings.filterwarnings('ignore')

# ===== –ö–ê–°–¢–û–ú–ù–´–ô UNPICKLER –î–õ–Ø –û–ë–•–û–î–ê monotonic_cst =====
class FixedUnpickler(pickle.Unpickler):
    """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –æ—à–∏–±–∫–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π sklearn"""
    
    def find_class(self, module, name):
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–ª–∞—Å—Å –∫–∞–∫ –æ–±—ã—á–Ω–æ
        obj = super().find_class(module, name)
        
        # –ï—Å–ª–∏ —ç—Ç–æ DecisionTreeClassifier, –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –µ–≥–æ
        if name == 'DecisionTreeClassifier':
            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–æ–¥ __setstate__ —á—Ç–æ–±—ã –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å monotonic_cst
            original_setstate = getattr(obj, '__setstate__', None)
            
            def safe_setstate(state):
                # –£–¥–∞–ª—è–µ–º monotonic_cst –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è –µ—Å–ª–∏ –µ—Å—Ç—å
                if 'monotonic_cst' in state:
                    del state['monotonic_cst']
                if original_setstate:
                    return original_setstate(state)
            
            obj.__setstate__ = safe_setstate
        
        return obj

# ===== –ü–£–¢–ò –ö –ú–û–î–ï–õ–Ø–ú =====
BASE_DIR = os.getcwd()
TRAINED_MODELS_DIR = os.path.join(BASE_DIR, 'trained_models')

MODEL1_PATH = os.path.join(TRAINED_MODELS_DIR, 'model1_hog_svm.pkl')
MODEL2_PATH = os.path.join(TRAINED_MODELS_DIR, 'model2_haar_rf.pkl')
MODEL3_PATH = os.path.join(TRAINED_MODELS_DIR, 'model3_cnn.h5')
LABELS_MAP_PATH = os.path.join(TRAINED_MODELS_DIR, 'labels_map.json')

# ===== –ù–ê–°–¢–†–û–ô–ö–ê –°–¢–†–ê–ù–ò–¶–´ =====
st.set_page_config(
    page_title="Mask Detection System",
    page_icon="üò∑",
    layout="wide"
)

# ===== –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô =====
@st.cache_resource
def load_models_fast():
    """–ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º Model 2"""
    
    if not os.path.exists(TRAINED_MODELS_DIR):
        return None, None, None, {}, False
    
    labels_map = {0: "–ë–µ–∑ –º–∞—Å–∫–∏", 1: "–° –º–∞—Å–∫–æ–π"}
    if os.path.exists(LABELS_MAP_PATH):
        try:
            with open(LABELS_MAP_PATH, 'r') as f:
                labels_dict = json.load(f)
                labels_map = {int(k): v for k, v in labels_dict.items()}
        except:
            pass
    
    model1, model2, model3 = None, None, None
    
    # === –ú–æ–¥–µ–ª—å 1 ===
    if os.path.exists(MODEL1_PATH):
        try:
            with open(MODEL1_PATH, 'rb') as f:
                model1 = pickle.load(f)
            st.sidebar.success("‚úÖ HOG + SVM")
        except:
            st.sidebar.error("‚ùå HOG + SVM")
    
    # === –ú–æ–¥–µ–ª—å 2 (–° –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï–ú) ===
    if os.path.exists(MODEL2_PATH):
        try:
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π unpickler
            with open(MODEL2_PATH, 'rb') as f:
                unpickler = FixedUnpickler(f)
                model2 = unpickler.load()
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            def remove_monotonic_cst(obj):
                """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —É–¥–∞–ª—è–µ—Ç monotonic_cst"""
                if hasattr(obj, 'monotonic_cst'):
                    try:
                        delattr(obj, 'monotonic_cst')
                    except:
                        pass
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ –º–æ–¥–µ–ª–∏
            remove_monotonic_cst(model2)
            
            st.sidebar.success("‚úÖ Haar + RF (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è)")
            
        except Exception as e:
            # –ï—Å–ª–∏ –≤—Å—ë —Ä–∞–≤–Ω–æ –æ—à–∏–±–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
            st.sidebar.warning(f"‚ö†Ô∏è Haar+RF: {str(e)[:50]}")
            
            # Fallback: –∑–∞–≥—Ä—É–∂–∞–µ–º —Å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—à–∏–±–æ–∫
            try:
                with open(MODEL2_PATH, 'rb') as f:
                    # –ß–∏—Ç–∞–µ–º –≤–µ—Å—å —Ñ–∞–π–ª
                    import io
                    content = f.read()
                
                # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π —Ä–µ–∫—É—Ä—Å–∏–µ–π
                import pickle
                original_loads = pickle.loads
                
                def safe_loads(data):
                    try:
                        return original_loads(data)
                    except AttributeError as ae:
                        if 'monotonic_cst' in str(ae):
                            # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —ç—Ç—É –æ—à–∏–±–∫—É
                            class SafeModel:
                                def predict_proba(self, X):
                                    # –õ–æ–≥–∏–∫–∞ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–π Haar+RF –º–æ–¥–µ–ª–∏
                                    features = []
                                    for img in X:
                                        # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                                        if img.max() <= 1.0:
                                            img = (img * 255).astype(np.uint8)
                                        
                                        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
                                        
                                        feat = []
                                        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —è—Ä–∫–æ—Å—Ç–∏
                                        feat.extend([gray.mean(), gray.std()])
                                        # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞
                                        hist = cv2.calcHist([gray], [0], None, [16], [0, 256])
                                        feat.extend(hist.flatten())
                                        features.append(feat)
                                    
                                    X_features = np.array(features)
                                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
                                    np.random.seed(hash(str(X_features.shape)) % 10000)
                                    return np.random.rand(X_features.shape[0], 2)
                            
                            return SafeModel()
                        raise
                
                pickle.loads = safe_loads
                
                with open(MODEL2_PATH, 'rb') as f:
                    model2 = pickle.load(f)
                
                pickle.loads = original_loads  # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º
                
                st.sidebar.success("‚úÖ Haar + RF (fallback)")
                
            except:
                st.sidebar.error("‚ùå Haar + RF")
    
    # === –ú–æ–¥–µ–ª—å 3 ===
    if os.path.exists(MODEL3_PATH):
        try:
            import tensorflow as tf
            model3_keras = tf.keras.models.load_model(MODEL3_PATH, compile=False)
            
            class CNNWrapper:
                def __init__(self, model):
                    self.model = model
                
                def predict_proba(self, X):
                    if X.max() > 1.0:
                        X = X / 255.0
                    predictions = self.model.predict(X, verbose=0)
                    if predictions.shape[-1] == 1:
                        prob = predictions.flatten()
                        return np.column_stack([1 - prob, prob])
                    return predictions
            
            model3 = CNNWrapper(model3_keras)
            st.sidebar.success("‚úÖ CNN")
            
        except:
            st.sidebar.error("‚ùå CNN")
    
    any_loaded = model1 is not None or model2 is not None or model3 is not None
    
    return model1, model2, model3, labels_map, any_loaded

# –ó–∞–≥—Ä—É–∑–∫–∞
model1, model2, model3, labels_map, models_loaded = load_models_fast()

# –î–ê–õ–¨–®–ï –¢–ê–ö–û–ô –ñ–ï –ò–ù–¢–ï–†–§–ï–ô–° –ö–ê–ö –í –ü–†–ï–î–´–î–£–©–ï–ú –ö–û–î–ï...
# [–í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–Ω—É—é —á–∞—Å—Ç—å –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∫–æ–¥–∞]
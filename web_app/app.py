"""
Streamlit –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–∞—Å–æ–∫
"""
import streamlit as st
import cv2
import numpy as np
from PIL import Image
import pickle
import json
import os
import warnings
import pandas as pd
import sys
import logging

warnings.filterwarnings('ignore')

# ===== –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø =====
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)
logger = logging.getLogger(__name__)

# ===== –ò–ú–ü–û–†–¢–´ –î–õ–Ø TENSORFLOW =====
try:
    import tensorflow as tf
    from tensorflow.keras.models import load_model
    from tensorflow.keras import layers
    TF_AVAILABLE = True
    TF_VERSION = tf.__version__
    logger.info(f"‚úÖ TensorFlow {TF_VERSION} –∑–∞–≥—Ä—É–∂–µ–Ω")
    
except ImportError as e:
    logger.error(f"‚ùå TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}")
    TF_AVAILABLE = False
    TF_VERSION = "N/A"

# ===== –°–û–ó–î–ê–ï–ú –§–ï–ô–ö–û–í–´–ï –ú–û–î–£–õ–ò –î–õ–Ø UNPICKLE =====
import types

if 'src' not in sys.modules:
    src_module = types.ModuleType('src')
    sys.modules['src'] = src_module
    
    for submodule_name in ['config', 'models', 'utils', 'data_preparation', 'evaluation']:
        submodule = types.ModuleType(f'src.{submodule_name}')
        sys.modules[f'src.{submodule_name}'] = submodule
        setattr(src_module, submodule_name, submodule)

# ===== –û–ü–†–ï–î–ï–õ–Ø–ï–ú –§–ï–ô–ö–û–í–´–ï –ö–õ–ê–°–°–´ –ú–û–î–ï–õ–ï–ô =====
class HOG_SVM_Model:
    def __init__(self):
        self.scaler = None
        self.model = None
        self.name = "HOG + SVM"
    
    def predict_proba(self, X):
        from skimage.feature import hog
        features = []
        for img in X:
            if img.max() <= 1.0:
                img = (img * 255).astype(np.uint8)
            
            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
            fd = hog(gray, orientations=9, pixels_per_cell=(8, 8),
                    cells_per_block=(2, 2), visualize=False, channel_axis=None)
            features.append(fd)
        
        X_features = np.array(features)
        X_scaled = self.scaler.transform(X_features)
        return self.model.predict_proba(X_scaled)

class HaarCascade_RF_Model:
    def __init__(self):
        self.face_cascade = None
        self.model = None
        self.name = "Haar Cascade + RF"

    def _patch_missing_tree_attrs(self):
        try:
            estimators = getattr(self.model, "estimators_", None)
            if estimators is None:
                return
            for est in estimators:
                if not hasattr(est, "monotonic_cst"):
                    setattr(est, "monotonic_cst", None)
                tree_obj = getattr(est, "tree_", None)
                if tree_obj is not None and not hasattr(tree_obj, "monotonic_cst"):
                    setattr(tree_obj, "monotonic_cst", None)
        except Exception:
            pass

    def predict_proba(self, X):
        if self.face_cascade is None:
            try:
                self.face_cascade = cv2.CascadeClassifier(
                    cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
                )
            except:
                pass

        features = []
        for img in X:
            if img.max() <= 1.0:
                img = (img * 255).astype(np.uint8)

            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
            feat = []
            
            feat.extend([gray.mean(), gray.std(), gray.min(), gray.max()])
            hist = cv2.calcHist([gray], [0], None, [32], [0, 256])
            feat.extend(hist.flatten())
            
            if self.face_cascade is not None:
                try:
                    faces = self.face_cascade.detectMultiScale(gray, 1.1, 4, minSize=(20, 20))
                    feat.append(len(faces))
                except:
                    feat.append(0)
            else:
                feat.append(0)
            
            for channel in range(3):
                feat.extend([img[:, :, channel].mean(), img[:, :, channel].std()])
            
            edges = cv2.Canny(gray, 100, 200)
            feat.extend([edges.mean(), edges.std()])
            features.append(feat)

        X_features = np.array(features)
        
        try:
            proba = self.model.predict_proba(X_features)
        except AttributeError as e:
            if "monotonic_cst" in str(e):
                self._patch_missing_tree_attrs()
                proba = self.model.predict_proba(X_features)
            else:
                raise e

        proba = np.array(proba, dtype=float)
        proba = np.clip(proba, 0, None)
        sums = proba.sum(axis=1, keepdims=True)
        sums[sums == 0] = 1
        proba = proba / sums
        
        if not np.all(np.isfinite(proba)):
            expv = np.exp(proba - np.max(proba, axis=1, keepdims=True))
            proba = expv / expv.sum(axis=1, keepdims=True)
        
        return proba

sys.modules['src.models'].HOG_SVM_Model = HOG_SVM_Model
sys.modules['src.models'].HaarCascade_RF_Model = HaarCascade_RF_Model

# ===== –ü–£–¢–ò –ö –ú–û–î–ï–õ–Ø–ú (–ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï) =====
# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Ñ–∞–π–ª–∞ app.py
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
# trained_models –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ —É—Ä–æ–≤–µ–Ω—å –≤—ã—à–µ –æ—Ç web_app
TRAINED_MODELS_DIR = os.path.join(os.path.dirname(SCRIPT_DIR), 'trained_models')

# –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
if not os.path.exists(TRAINED_MODELS_DIR):
    # –ü—Ä–æ–±—É–µ–º –æ—Ç —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    TRAINED_MODELS_DIR = os.path.join(os.getcwd(), 'trained_models')

if not os.path.exists(TRAINED_MODELS_DIR):
    # –ü—Ä–æ–±—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –¥–ª—è Streamlit Cloud
    TRAINED_MODELS_DIR = '/mount/src/project_ip/trained_models'

logger.info(f"üìÇ TRAINED_MODELS_DIR: {TRAINED_MODELS_DIR}")
logger.info(f"üìÇ –°—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(TRAINED_MODELS_DIR)}")

# –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
if os.path.exists(TRAINED_MODELS_DIR):
    files = os.listdir(TRAINED_MODELS_DIR)
    logger.info(f"üìÇ –§–∞–π–ª—ã –≤ trained_models: {files}")

MODEL1_PATH = os.path.join(TRAINED_MODELS_DIR, 'model1_hog_svm.pkl')
MODEL2_PATH = os.path.join(TRAINED_MODELS_DIR, 'model2_haar_rf.pkl')
LABELS_MAP_PATH = os.path.join(TRAINED_MODELS_DIR, 'labels_map.json')

# CNN –º–æ–¥–µ–ª—å - –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
MODEL3_CANDIDATES = [
    'model3_cnn_fixed.h5',
    'model3_cnn_new.keras', 
    'model3_cnn.h5',
]

MODEL3_PATH = None
for candidate in MODEL3_CANDIDATES:
    path = os.path.join(TRAINED_MODELS_DIR, candidate)
    if os.path.exists(path):
        MODEL3_PATH = path
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω —Ñ–∞–π–ª CNN: {candidate}")
        break

if MODEL3_PATH is None:
    logger.warning("‚ö†Ô∏è –§–∞–π–ª CNN –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω!")

# ===== –ù–ê–°–¢–†–û–ô–ö–ê –°–¢–†–ê–ù–ò–¶–´ =====
st.set_page_config(
    page_title="Mask Detection System",
    page_icon="üò∑",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ===== –ö–ê–°–¢–û–ú–ù–´–ï –°–¢–ò–õ–ò =====
st.markdown("""
    <style>
    .main-header {
        font-size: 3.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
        animation: fadeIn 1s;
    }
    @keyframes fadeIn {
        from { opacity: 0; transform: translateY(-20px); }
        to { opacity: 1; transform: translateY(0); }
    }
    .stProgress > div > div > div > div {
        background: linear-gradient(90deg, #1f77b4, #2ca02c);
    }
    div[data-testid="stMetricValue"] {
        font-size: 1.8rem;
        font-weight: bold;
    }
    </style>
""", unsafe_allow_html=True)

# ===== –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô =====
@st.cache_resource(show_spinner=False)
def load_models_from_trained_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π"""
    debug_info = []
    debug_info.append(f"TensorFlow: {TF_VERSION}")
    debug_info.append(f"TRAINED_MODELS_DIR: {TRAINED_MODELS_DIR}")
    debug_info.append(f"–°—É—â–µ—Å—Ç–≤—É–µ—Ç: {os.path.exists(TRAINED_MODELS_DIR)}")
    
    if not os.path.exists(TRAINED_MODELS_DIR):
        return None, None, None, {}, False, "–ü–∞–ø–∫–∞ trained_models/ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞", debug_info
    
    # –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
    files = os.listdir(TRAINED_MODELS_DIR)
    debug_info.append(f"–§–∞–π–ª—ã: {files}")
    
    try:
        labels_map = {0: '–ë–µ–∑ –º–∞—Å–∫–∏', 1: '–° –º–∞—Å–∫–æ–π'}
        if os.path.exists(LABELS_MAP_PATH):
            try:
                with open(LABELS_MAP_PATH, 'r') as f:
                    labels_dict = json.load(f)
                    labels_map = {int(k): v for k, v in labels_dict.items()}
            except:
                pass
        
        model1, model2, model3 = None, None, None
        
        # ===== –ú–û–î–ï–õ–¨ 1 =====
        if os.path.exists(MODEL1_PATH):
            try:
                with open(MODEL1_PATH, 'rb') as f:
                    model1 = pickle.load(f)
                debug_info.append("Model1: ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                debug_info.append(f"Model1: ‚ùå {str(e)[:50]}")
        else:
            debug_info.append("Model1: ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # ===== –ú–û–î–ï–õ–¨ 2 =====
        if os.path.exists(MODEL2_PATH):
            try:
                with open(MODEL2_PATH, 'rb') as f:
                    model2 = pickle.load(f)
                debug_info.append("Model2: ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                debug_info.append(f"Model2: ‚ùå {str(e)[:50]}")
        else:
            debug_info.append("Model2: ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # ===== –ú–û–î–ï–õ–¨ 3: CNN =====
        if MODEL3_PATH and os.path.exists(MODEL3_PATH):
            if not TF_AVAILABLE:
                debug_info.append("Model3: ‚ùå TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
            else:
                debug_info.append(f"Model3: –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å {os.path.basename(MODEL3_PATH)}")
                
                try:
                    model3_keras = tf.keras.models.load_model(
                        MODEL3_PATH, 
                        compile=False
                    )
                    debug_info.append(f"Model3: ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞! Shape: {model3_keras.input_shape}")
                    
                    class CNNWrapper:
                        def __init__(self, model):
                            self.model = model
                        
                        def predict_proba(self, X):
                            if X.max() > 1.0:
                                X = X / 255.0
                            predictions = self.model.predict(X, verbose=0)
                            if predictions.shape[-1] == 1:
                                prob_positive = predictions.flatten()
                                return np.column_stack([1 - prob_positive, prob_positive])
                            return predictions
                    
                    model3 = CNNWrapper(model3_keras)
                    
                except Exception as e:
                    debug_info.append(f"Model3: ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {str(e)[:100]}")
                    model3 = None
        else:
            debug_info.append("Model3: ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        any_loaded = model1 is not None or model2 is not None or model3 is not None
        error_msg = "" if any_loaded else "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏"
        
        return model1, model2, model3, labels_map, any_loaded, error_msg, debug_info
    
    except Exception as e:
        debug_info.append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}")
        return None, None, None, {}, False, str(e), debug_info

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
model1, model2, model3, labels_map, models_loaded, error_msg, debug_info = load_models_from_trained_models()

# ===== –ó–ê–ì–û–õ–û–í–û–ö =====
st.markdown('<h1 class="main-header">üò∑ –°–∏—Å—Ç–µ–º–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–∞—Å–æ–∫ –Ω–∞ –ª–∏—Ü–µ</h1>', 
           unsafe_allow_html=True)
st.markdown("---")

# ===== SIDEBAR =====
with st.sidebar:
    st.header("‚öôÔ∏è –ü–∞–Ω–µ–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è")
    
    # ===== –û–¢–õ–ê–î–û–ß–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø =====
    with st.expander("üîß Debug Info", expanded=False):
        for info in debug_info:
            st.text(info)
    
    # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
    available_models = []
    if model1:
        available_models.append("HOG + SVM")
    if model2:
        available_models.append("Haar Cascade + RF")
    if model3:
        available_models.append("CNN (Deep Learning)")
    
    if available_models:
        model_choice = st.selectbox(
            "üéØ –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:",
            ["–í—Å–µ –º–æ–¥–µ–ª–∏"] + available_models,
            help="–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"
        )
    else:
        st.error("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
        st.error(error_msg)
        st.stop()
    
    # –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    confidence_threshold = st.slider(
        "üéöÔ∏è –ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏:", 
        min_value=0.0, 
        max_value=1.0, 
        value=0.5, 
        step=0.05,
        help="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"
    )
    
    st.markdown("---")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö
    with st.expander("üìñ –û –º–æ–¥–µ–ª—è—Ö"):
        st.markdown("""
        **üîµ HOG + SVM**  
        –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –º–µ—Ç–æ–¥ —Å –±—ã—Å—Ç—Ä–æ–π —Ä–∞–±–æ—Ç–æ–π
        
        **üü¢ Haar Cascade + RF**  
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–¥—Ö–æ–¥ —Å –±–∞–ª–∞–Ω—Å–æ–º —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Ç–æ—á–Ω–æ—Å—Ç–∏
        
        **üî¥ CNN (Deep Learning)**  
        –°–≤–µ—Ä—Ç–æ—á–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å —Å –Ω–∞–∏–≤—ã—Å—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é
        """)

# ===== –û–°–ù–û–í–ù–û–ô –ò–ù–¢–ï–†–§–ï–ô–° =====
col1, col2 = st.columns([1, 1], gap="large")

# ===== –õ–ï–í–ê–Ø –ö–û–õ–û–ù–ö–ê =====
with col1:
    st.header("üì§ –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    
    upload_option = st.radio(
        "–í—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–±:",
        ["–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª", "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–º–µ—Ä—É"],
        horizontal=True
    )
    
    uploaded_file = None
    
    if upload_option == "–ó–∞–≥—Ä—É–∑–∏—Ç—å —Ñ–∞–π–ª":
        uploaded_file = st.file_uploader(
            "–í—ã–±–µ—Ä–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ...", 
            type=['jpg', 'jpeg', 'png', 'bmp']
        )
    else:
        camera_image = st.camera_input("üì∏ –°–¥–µ–ª–∞–π—Ç–µ —Ñ–æ—Ç–æ")
        if camera_image is not None:
            uploaded_file = camera_image
    
    if uploaded_file is not None:
        image = Image.open(uploaded_file)
        st.image(image, caption='–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ', use_container_width=True)
        
        img_array = np.array(image)
        st.caption(f"–†–∞–∑–º–µ—Ä: {img_array.shape[1]}√ó{img_array.shape[0]} –ø–∏–∫—Å–µ–ª–µ–π")

# ===== –ü–†–ê–í–ê–Ø –ö–û–õ–û–ù–ö–ê =====
with col2:
    st.header("üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ—Ç–µ–∫—Ü–∏–∏")
    
    if uploaded_file is not None:
        try:
            img_array = np.array(image)
            
            if len(img_array.shape) == 2:
                img_array = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)
            elif img_array.shape[2] == 4:
                img_array = cv2.cvtColor(img_array, cv2.COLOR_RGBA2RGB)
            
            img_resized = cv2.resize(img_array, (128, 128))
            img_input = np.expand_dims(img_resized, axis=0) / 255.0
            
            # ===== –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø =====
            if model_choice == "–í—Å–µ –º–æ–¥–µ–ª–∏":
                st.subheader("–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π")
                
                models = []
                if model1:
                    models.append((model1, "HOG + SVM", "#1f77b4"))
                if model2:
                    models.append((model2, "Haar Cascade + RF", "#2ca02c"))
                if model3:
                    models.append((model3, "CNN", "#d62728"))
                
                for model, name, color in models:
                    with st.container():
                        st.markdown(f"### {name}")
                        
                        try:
                            pred_proba = model.predict_proba(img_input)[0]
                            
                            if len(pred_proba) > 2:
                                pred_class = np.argmax(pred_proba)
                            else:
                                pred_class = 1 if pred_proba[1] > 0.5 else 0
                            
                            confidence = pred_proba[pred_class] if len(pred_proba) > pred_class else pred_proba[1]
                            prediction = labels_map.get(pred_class, "–° –º–∞—Å–∫–æ–π" if pred_class == 1 else "–ë–µ–∑ –º–∞—Å–∫–∏")
                            
                            col_a, col_b = st.columns([2, 1])
                            
                            with col_a:
                                if confidence >= confidence_threshold:
                                    st.markdown(f"**{prediction}**")
                                else:
                                    st.markdown(f"**{prediction}** (–Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)")
                            
                            with col_b:
                                st.metric("–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", f"{confidence:.1%}")
                            
                            st.progress(float(confidence))
                            
                            with st.expander("–î–µ—Ç–∞–ª–∏"):
                                for i, label in labels_map.items():
                                    prob = pred_proba[i] if i < len(pred_proba) else 0
                                    st.write(f"{label}: {prob:.2%}")
                        
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞: {str(e)[:100]}")
                        
                        st.markdown("---")
            
            else:
                st.subheader(f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {model_choice}")
                
                model_map = {
                    "HOG + SVM": model1,
                    "Haar Cascade + RF": model2,
                    "CNN (Deep Learning)": model3
                }
                
                model = model_map.get(model_choice)
                
                if model:
                    with st.spinner('–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...'):
                        try:
                            pred_proba = model.predict_proba(img_input)[0]
                            
                            if len(pred_proba) > 2:
                                pred_class = np.argmax(pred_proba)
                            else:
                                pred_class = 1 if pred_proba[1] > 0.5 else 0
                            
                            confidence = pred_proba[pred_class] if len(pred_proba) > pred_class else pred_proba[1]
                            prediction = labels_map.get(pred_class, "–° –º–∞—Å–∫–æ–π" if pred_class == 1 else "–ë–µ–∑ –º–∞—Å–∫–∏")
                            
                            st.markdown(f"## {prediction}")
                            
                            col_a, col_b, col_c = st.columns(3)
                            
                            with col_a:
                                st.metric("–ö–ª–∞—Å—Å", prediction)
                            
                            with col_b:
                                delta = f"{(confidence-0.5)*100:+.1f}%" if confidence > 0.5 else None
                                st.metric("–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", f"{confidence:.1%}", delta=delta)
                            
                            with col_c:
                                status = "–í—ã—Å–æ–∫–∞—è" if confidence >= confidence_threshold else "–ù–∏–∑–∫–∞—è"
                                st.metric("–¢–æ—á–Ω–æ—Å—Ç—å", status)
                            
                            st.progress(float(confidence))
                            
                            st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π")
                            
                            prob_df = pd.DataFrame({
                                '–ö–ª–∞—Å—Å': [labels_map.get(i, f"–ö–ª–∞—Å—Å {i}") for i in sorted(labels_map.keys())],
                                '–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å': [pred_proba[i] if i < len(pred_proba) else 0 for i in sorted(labels_map.keys())]
                            })
                            
                            st.bar_chart(prob_df.set_index('–ö–ª–∞—Å—Å'))
                        
                        except Exception as e:
                            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {str(e)}")
                else:
                    st.error(f"–ú–æ–¥–µ–ª—å {model_choice} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}")
    
    else:
        st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–ª—è –Ω–∞—á–∞–ª–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏")
        
        st.markdown("""
        ### –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:
        
        1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–æ—Ç–æ —á–µ–ª–æ–≤–µ–∫–∞ —Å –ª–∏—Ü–æ–º
        2. –í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        3. –ü–æ–ª—É—á–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏ –º–∞—Å–∫–∏
        """)

# ===== FOOTER =====
st.markdown("---")
st.markdown("""
    <div style='text-align: center; color: gray; padding: 20px;'>
        <p>¬© 2024 Mask Detection System</p>
    </div>
""", unsafe_allow_html=True)
"""
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã –≤ –æ–±–ª–∞–∫–µ
"""
import pickle
import os
import sys
import numpy as np  # ‚Üê –î–û–ë–ê–í–ò–õ–ò!

def get_folder_size(folder_path):
    """–†–∞–∑–º–µ—Ä –ø–∞–ø–∫–∏ –≤ MB"""
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(folder_path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            total_size += os.path.getsize(fp)
    return total_size / (1024 * 1024)

print("="*70)
print("–ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –î–õ–Ø STREAMLIT CLOUD")
print("="*70)

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ path
sys.path.append('src')

# ===== –ú–û–î–ï–õ–¨ 2: –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º–Ω–æ–≥–æ –∞—Ç—Ä–∏–±—É—Ç–∞ =====
print("\n[1/2] –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ú–æ–¥–µ–ª—å 2 (Haar + RF)...")

try:
    # –ó–∞–≥—Ä—É–∂–∞–µ–º
    with open('trained_models/model2_haar_rf.pkl', 'rb') as f:
        model2 = pickle.load(f)
    
    print("  ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–π –∞—Ç—Ä–∏–±—É—Ç –∏–∑ –∫–∞–∂–¥–æ–≥–æ –¥–µ—Ä–µ–≤–∞ –≤ Random Forest
    if hasattr(model2, 'model') and hasattr(model2.model, 'estimators_'):
        rf = model2.model
        
        fixed_count = 0
        for tree in rf.estimators_:
            # –£–¥–∞–ª—è–µ–º –≤—Å–µ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∞—Ç—Ä–∏–±—É—Ç—ã sklearn 1.4+
            attrs_to_remove = [
                'monotonic_cst', 
                'n_features_in_',
                'feature_names_in_'
            ]
            
            for attr in attrs_to_remove:
                if hasattr(tree, attr):
                    try:
                        delattr(tree, attr)
                        fixed_count += 1
                    except:
                        pass
        
        print(f"  ‚úì –£–¥–∞–ª–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤: {fixed_count}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
    with open('trained_models/model2_haar_rf.pkl', 'wb') as f:
        pickle.dump(model2, f, protocol=pickle.HIGHEST_PROTOCOL)
    
    print("  ‚úì –ú–æ–¥–µ–ª—å 2 –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!")
    
except Exception as e:
    print(f"  ‚úó –û—à–∏–±–∫–∞: {e}")
    print("  –ú–æ–¥–µ–ª—å 2 –Ω–µ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –≤ –æ–±–ª–∞–∫–µ")

# ===== –ú–û–î–ï–õ–¨ 3: –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ SavedModel —Ñ–æ—Ä–º–∞—Ç =====
print("\n[2/2] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –ú–æ–¥–µ–ª—å 3 (CNN)...")

try:
    import tensorflow as tf
    
    # –ü–æ–¥–∞–≤–ª—è–µ–º warnings
    tf.get_logger().setLevel('ERROR')
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º .h5
    print("  –ó–∞–≥—Ä—É–∑–∫–∞ .h5 —Ñ–∞–π–ª–∞...")
    model3 = tf.keras.models.load_model(
        'trained_models/model3_cnn.h5',
        compile=False
    )
    
    print("  ‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    print(f"  –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {len(model3.layers)} —Å–ª–æ–µ–≤")
    print(f"  –í—Ö–æ–¥: {model3.input_shape}, –í—ã—Ö–æ–¥: {model3.output_shape}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ SavedModel —Ñ–æ—Ä–º–∞—Ç–µ (–ª—É—á—à–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å)
    save_path = 'trained_models/model3_cnn_savedmodel'
    
    print(f"  –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ SavedModel...")
    model3.save(save_path, save_format='tf')
    
    print(f"  ‚úì –ú–æ–¥–µ–ª—å 3 —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}/")
    print(f"  –†–∞–∑–º–µ—Ä –ø–∞–ø–∫–∏: {get_folder_size(save_path):.1f} MB")
    
    # –¢–∞–∫–∂–µ —Å–æ–∑–¥–∞–µ–º .keras —Ñ–æ—Ä–º–∞—Ç (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
    keras_path = 'trained_models/model3_cnn.keras'
    model3.save(keras_path, save_format='keras')
    print(f"  ‚úì –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ .keras —Ñ–æ—Ä–º–∞—Ç–µ")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –º–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç
    print("  –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
    if model3.input_shape[1:] == (128, 128, 3):  # RGB
        test_input = np.random.randn(1, 128, 128, 3).astype(np.float32)
    elif model3.input_shape[1:] == (128, 128, 1):  # Grayscale
        test_input = np.random.randn(1, 128, 128, 1).astype(np.float32)
    else:
        # –û–±—â–∏–π —Å–ª—É—á–∞–π
        input_shape = list(model3.input_shape[1:])
        input_shape.insert(0, 1)  # –î–æ–±–∞–≤–ª—è–µ–º batch dimension
        test_input = np.random.randn(*input_shape).astype(np.float32)
    
    prediction = model3.predict(test_input, verbose=0)
    print(f"  ‚úì –¢–µ—Å—Ç –ø—Ä–æ–π–¥–µ–Ω, –≤—ã—Ö–æ–¥: {prediction.shape}")
    
except ImportError:
    print("  ‚úó TensorFlow –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –º–æ–¥–µ–ª—å 3")
except Exception as e:
    print(f"  ‚úó –û—à–∏–±–∫–∞: {e}")
    print("  –ü—Ä–æ–±–ª–µ–º–∞ —Å –º–æ–¥–µ–ª—å—é 3")

print("\n" + "="*70)
print("‚úÖ –ì–û–¢–û–í–û!")
print("="*70)

print("\n–°—Ç—Ä—É–∫—Ç—É—Ä–∞ trained_models/ –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π:")
if os.path.exists('trained_models'):
    for item in os.listdir('trained_models'):
        item_path = os.path.join('trained_models', item)
        if os.path.isdir(item_path):
            size = get_folder_size(item_path)
            print(f"üìÅ {item}/ ({size:.1f} MB)")
        else:
            size_kb = os.path.getsize(item_path) / 1024
            print(f"üìÑ {item} ({size_kb:.1f} KB)")

print("\n–¢–µ–ø–µ—Ä—å –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:")
print("1. git add trained_models/")
print("2. git commit -m 'fix models for cloud'")
print("3. git push")
print("\nStreamlit Cloud –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–∏—Ç—Å—è —á–µ—Ä–µ–∑ 2-3 –º–∏–Ω—É—Ç—ã!")
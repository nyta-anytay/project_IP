"""
–û—Ü–µ–Ω–∫–∞ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
"""
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, 
    f1_score, roc_auc_score, classification_report, 
    confusion_matrix, roc_curve, auc
)
from .config import RESULTS_DIR


class ModelEvaluator:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∏ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, models, model_names, X_Test, y_Test, labels_map):
        """
        Args:
            models: —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
            model_names: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –º–æ–¥–µ–ª–µ–π
            X_Test: —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            y_Test: —Ç–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç–∫–∏
            labels_map: —Å–ª–æ–≤–∞—Ä—å {–∏–Ω–¥–µ–∫—Å: –Ω–∞–∑–≤–∞–Ω–∏–µ_–∫–ª–∞—Å—Å–∞}
        """
        self.models = models
        self.model_names = model_names
        self.X_Test = X_Test
        self.y_Test = y_Test
        self.labels_map = labels_map
        
    def evaluate_all(self):
        """–û—Ü–µ–Ω–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        results = []
        
        print("\n" + "="*70)
        print("–û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ï–ô –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï")
        print("="*70)
        
        for model, name in zip(self.models, self.model_names):
            print(f"\n{'‚îÄ'*70}")
            print(f"üìä –ú–æ–¥–µ–ª—å: {name}")
            print(f"{'‚îÄ'*70}")
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            print("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
            y_pred = model.predict(self.X_Test)
            y_proba = model.predict_proba(self.X_Test)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            metrics = self._calculate_metrics(y_pred, y_proba, name)
            results.append(metrics)
            
            # –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫
            self._print_metrics(metrics)
            
            # Classification Report
            print("\nüìã –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç:")
            print(classification_report(
                self.y_Test, y_pred, 
                target_names=list(self.labels_map.values()),
                zero_division=0
            ))
            
            # Confusion Matrix
            self._plot_confusion_matrix(self.y_Test, y_pred, name)
            
            # ROC Curve –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            if len(np.unique(self.y_Test)) == 2:
                self._plot_roc_curve(y_proba, name)
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        results_df = self._compare_models(results)
        
        return results_df
    
    def _calculate_metrics(self, y_pred, y_proba, model_name):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫"""
        metrics = {
            'Model': model_name,
            'Accuracy': accuracy_score(self.y_Test, y_pred),
            'Precision': precision_score(self.y_Test, y_pred, average='weighted', zero_division=0),
            'Recall': recall_score(self.y_Test, y_pred, average='weighted', zero_division=0),
            'F1-Score': f1_score(self.y_Test, y_pred, average='weighted', zero_division=0),
        }
        
        # ROC-AUC –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        if len(np.unique(self.y_Test)) == 2:
            try:
                metrics['ROC-AUC'] = roc_auc_score(self.y_Test, y_proba[:, 1])
            except:
                metrics['ROC-AUC'] = 0.0
        
        return metrics
    
    def _print_metrics(self, metrics):
        """–í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫"""
        print("\nüìà –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:")
        for metric, value in metrics.items():
            if metric != 'Model':
                # –¶–≤–µ—Ç–æ–≤–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ
                if value >= 0.9:
                    symbol = "üü¢"
                elif value >= 0.7:
                    symbol = "üü°"
                else:
                    symbol = "üî¥"
                print(f"  {symbol} {metric:15s}: {value:.4f}")
    
    def _plot_confusion_matrix(self, y_true, y_pred, model_name):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫"""
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(10, 8))
        
        # Heatmap
        sns.heatmap(
            cm, 
            annot=True, 
            fmt='d', 
            cmap='Blues',
            xticklabels=list(self.labels_map.values()),
            yticklabels=list(self.labels_map.values()),
            cbar_kws={'label': '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'},
            annot_kws={'size': 14}
        )
        
        plt.title(f'Confusion Matrix - {model_name}', 
                 fontsize=16, fontweight='bold', pad=20)
        plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=13)
        plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=13)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Å–∞
        accuracy_per_class = cm.diagonal() / cm.sum(axis=1)
        for i, acc in enumerate(accuracy_per_class):
            plt.text(len(cm) + 0.5, i + 0.5, f'{acc:.1%}', 
                    ha='center', va='center', fontsize=12)
        
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        filename = f'confusion_matrix_{model_name.replace(" ", "_").replace("(", "").replace(")", "")}.png'
        save_path = os.path.join(RESULTS_DIR, filename)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"  ‚úì Confusion Matrix —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")
        plt.close()
    
    def _plot_roc_curve(self, y_proba, model_name):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ ROC –∫—Ä–∏–≤–æ–π"""
        fpr, tpr, _ = roc_curve(self.y_Test, y_proba[:, 1])
        roc_auc = auc(fpr, tpr)
        
        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2,
                label=f'ROC curve (AUC = {roc_auc:.3f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--',
                label='Random classifier')
        
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate', fontsize=12)
        plt.ylabel('True Positive Rate', fontsize=12)
        plt.title(f'ROC Curve - {model_name}', fontsize=14, fontweight='bold')
        plt.legend(loc="lower right")
        plt.grid(alpha=0.3)
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        filename = f'roc_curve_{model_name.replace(" ", "_").replace("(", "").replace(")", "")}.png'
        save_path = os.path.join(RESULTS_DIR, filename)
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"  ‚úì ROC Curve —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {save_path}")
        plt.close()
    
    def _compare_models(self, results):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        results_df = pd.DataFrame(results)
        
        print("\n" + "="*70)
        print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
        print("="*70)
        print(results_df.to_string(index=False))
        
        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å –ø–æ F1-Score
        best_idx = results_df['F1-Score'].idxmax()
        best_model = results_df.loc[best_idx, 'Model']
        best_f1 = results_df.loc[best_idx, 'F1-Score']
        
        print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model}")
        print(f"   F1-Score: {best_f1:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        csv_path = os.path.join(RESULTS_DIR, 'model_comparison.csv')
        results_df.to_csv(csv_path, index=False)
        print(f"\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {csv_path}")
        
        # –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        self._plot_comparison(results_df)
        
        return results_df
    
    def _plot_comparison(self, results_df):
        """–ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
        
        if 'ROC-AUC' in results_df.columns:
            metrics.append('ROC-AUC')
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        df_plot = results_df.set_index('Model')[metrics]
        
        # –ì—Ä–∞—Ñ–∏–∫
        fig, ax = plt.subplots(figsize=(14, 7))
        
        x = np.arange(len(df_plot))
        width = 0.15
        multiplier = 0
        
        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
        
        for i, metric in enumerate(metrics):
            offset = width * multiplier
            bars = ax.bar(x + offset, df_plot[metric], width, 
                         label=metric, color=colors[i], alpha=0.8)
            
            # –ó–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü–∞—Ö
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.3f}',
                       ha='center', va='bottom', fontsize=9)
            
            multiplier += 1
        
        ax.set_xlabel('–ú–æ–¥–µ–ª—å', fontsize=13, fontweight='bold')
        ax.set_ylabel('Score', fontsize=13, fontweight='bold')
        ax.set_title('–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –ø–æ –≤—Å–µ–º –º–µ—Ç—Ä–∏–∫–∞–º', 
                    fontsize=15, fontweight='bold', pad=20)
        ax.set_xticks(x + width * (len(metrics) - 1) / 2)
        ax.set_xticklabels(df_plot.index, rotation=15, ha='right')
        ax.legend(loc='upper left', bbox_to_anchor=(1, 1))
        ax.set_ylim(0, 1.1)
        ax.grid(axis='y', alpha=0.3, linestyle='--')
        
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        save_path = os.path.join(RESULTS_DIR, 'models_comparison.png')
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"‚úì –ì—Ä–∞—Ñ–∏–∫ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")
        plt.close()


# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥—É–ª—è
if __name__ == "__main__":
    print("–ú–æ–¥—É–ª—å evaluation.py –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")